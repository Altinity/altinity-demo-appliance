#rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch
# Add the following in your /etc/yum.repos.d/ directory in a file with a .repo suffix, for example logstash.repo
#
# [logstash-5.x]
# name=Elastic repository for 5.x packages
# baseurl=https://artifacts.elastic.co/packages/5.x/yum
# gpgcheck=1
# gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch
# enabled=1
# autorefresh=1
# type=rpm-md
# yum install java-1.8.0-openjdk
# JAVA_HOME=/usr/lib/jvm/jre-1.8.0-openjdk.x86_64
# export JAVA_HOME
#require prune (logstash-filter-prune), multinline (logstash-filter-multiline) and clickhouse (logstash-output-clickhouse)
# clickhouse: https://github.com/mikechris/logstash-output-clickhouse
# /usr/share/logstash/bin/logstash-plugin install logstash-filter-multiline

# run logstash: /usr/share/logstash/bin/logstash -f logstash.conf     ||  initctl start logstash
# run filebeat: /usr/share/filebeat/bin/filebeat -e -c beat.yml


input {

  #stdin { }
  beats {
    port => 5044
    host => "127.0.0.1"
  }
}

filter {
  if [source] =~ "access_" {
      grok {
        match => [ "message" , "%{COMBINEDAPACHELOG}+%{GREEDYDATA:extra_fields}"]
      }

      mutate {
        convert => ["response", "integer"]
        convert => ["bytes", "integer"]
        convert => ["responsetime", "float"]
        remove_field => ["@version", "host", "message", "beat", "offset", "type", "tags", "input_type", "source"]
      }

      date {
        match => [ "timestamp" , "dd/MMM/YYYY:HH:mm:ss Z" ]
        remove_field => [ "timestamp", "@timestamp" ]
        target => [ "logdatetime" ]
      }

      ruby {
        code => "tstamp = event.get('logdatetime').to_i
                 event.set('logdatetime', Time.at(tstamp).strftime('%Y-%m-%d %H:%M:%S'))
                 event.set('logdate', Time.at(tstamp).strftime('%Y-%m-%d'))"
      }

      useragent {
        source => "agent"
      }

      prune {
          interpolate => true
          whitelist_names => ["^logdate$" ,"^logdatetime$" ,"^request$" ,"^agent$" ,"^os$" ,"^minor$" ,"^auth$" ,"^ident$" ,"^verb$" ,"^patch$" ,"^referrer$" ,"^major$" ,"^build$" ,"^response$","^bytes$","^clientip$" ,"^name$" ,"^os_name$" ,"^httpversion$" ,"^device$" ]
        }
  }

  if [source] =~ "mysql_" {
    grok {
        match => {
            message => [
                "# Time: \d+ \d\d:\d\d:\d\d",
                "# User@Host: %{USER:user}\[[^\]]+\] @ %{IPORHOST:lhost} \[%{IP:ip}?\]",
                "# Thread_id: .+",
                "# Query_time: %{NUMBER:duration:float} \s*Lock_time: %{NUMBER:lock_wait:float} \s*Rows_sent: %{NUMBER:results:int} \s*Rows_examined: %{NUMBER:scanned:int}",
                "SET timestamp=%{NUMBER:timestamp};",
                "%{GREEDYDATA:query}"
            ]
        }
    }

    multiline {
        pattern => "^# Time:"
        negate => true
        what => "previous"
    }

    date {
        # use the value from SET timestamp as the timestamp of the event
        match => [ "timestamp","UNIX" ]
        remove_field => [ "timestamp", "@timestamp" ]
        target => [ "logdatetime" ]
    }
    ruby {
        code => "tstamp = event.get('logdatetime').to_i
                 event.set('logdatetime', Time.at(tstamp).strftime('%Y-%m-%d %H:%M:%S'))
                 event.set('logdate', Time.at(tstamp).strftime('%Y-%m-%d'))"
      }
    mutate {
        join => { "query" => " " }
      }

    prune {
      interpolate => true
      whitelist_names => ["^logdate$", "logdatetime", "^query" ,"^duration$" ,"^lhost$" ,"^scanned$" ,"^lock_wait$" ,"^user$" ,"^results$" ]
    }
  }
}

output {
  if [request] {
    clickhouse {
      http_hosts => ["http://localhost:8123"]
      table => "nginx_access"
      request_tolerance => 1
      flush_size => 1000
      pool_max => 1000
    }
  }

  if ![request] and [duration] {
    clickhouse {
      http_hosts => ["http://localhost:8123"]
      table => "mysql_slow_log"
      request_tolerance => 1
      flush_size => 1000
      pool_max => 1000
    }
  }
}